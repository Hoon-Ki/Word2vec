{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, \n",
      "and 50000 unlabeled reviews\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "train = pd.read_csv( \"data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv( \"data/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"data/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# 데이터 행개수 확인 (총 100,000 개)\n",
    "print ((\"Read {0} labeled train reviews, {1} labeled test reviews, \"+ '\\n' +\"and {2} unlabeled reviews\").format(train[\"review\"].size, test[\"review\"].size, unlabeled_train[\"review\"].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering: 396.7486095428467seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time() # Start time\n",
    "\n",
    "# K를 vocabulary 길이의 1/5 로 클러스터수 설정, 즉 클러스터당 5개 단어들의 평균\n",
    "word_vectors = model.wv.syn0\n",
    "num_clusters = int(word_vectors.shape[0]/5)\n",
    "\n",
    "# 중심점을 추출하기 위해, K-means 객체를 초기화\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# 훈련시간 측정\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print (\"Time taken for K Means clustering: \"+ str(elapsed) + \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16490, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3298"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(word_vectors.shape[0]/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 단어 / 인덱스 사전 생성, 각 vocab에 있는 단어를 클러스터 번호에 맵핑                                        \n",
    "word_centroid_map = dict(zip( model.wv.index2word, idx ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['enthusiast', 'fanatic', 'vegetarian', 'snob', 'masochist', 'believer']\n",
      "\n",
      "Cluster 1\n",
      "['wolfe', 'sheen', 'julian', 'mr', 'morris']\n",
      "\n",
      "Cluster 2\n",
      "['team', 'unit', 'ops']\n",
      "\n",
      "Cluster 3\n",
      "['apathetic', 'unreasonable']\n",
      "\n",
      "Cluster 4\n",
      "['deceit', 'greed', 'primal']\n",
      "\n",
      "Cluster 5\n",
      "['diego', 'miguel', 'flamboyant']\n",
      "\n",
      "Cluster 6\n",
      "['spirituality', 'masculinity', 'diatribe', 'accusations', 'degradation', 'strain', 'harshness', 'perversion', 'depravity', 'anarchy', 'manipulation', 'cruelty', 'unspeakable', 'pretension', 'ramblings', 'awakening', 'indifference', 'bondage', 'delusion', 'baggage']\n",
      "\n",
      "Cluster 7\n",
      "['skull', 'mask', 'bat', 'cap', 'worm']\n",
      "\n",
      "Cluster 8\n",
      "['conquest', 'celebration', 'antichrist', 'creation']\n",
      "\n",
      "Cluster 9\n",
      "['accuses', 'confessed', 'assumes', 'admits', 'proposal', 'admitting', 'recognizes', 'succumbs', 'denies', 'declares', 'reacts', 'fakes', 'responds', 'swears', 'argues', 'confronts', 'confesses', 'imagines', 'resigned', 'listens', 'expresses']\n"
     ]
    }
   ],
   "source": [
    "# 첫 10개의 클러스터들에 대해서\n",
    "for cluster in range(0,10):\n",
    "    \n",
    "    # 클러스터 번호 출력\n",
    "    print (\"\\nCluster %d\" % cluster)\n",
    "    \n",
    "    # 모든 단어들의 해당 클러스터(1~10)번호를 찾고 출력\n",
    "    words = []\n",
    "    for i in range(0,len(word_centroid_map.values())):        # 클러스터 번호 수 만큼 루프문\n",
    "        if(list(word_centroid_map.values())[i] == cluster):   # 해당 클러스터가 1~10 차례의 클러스터 번호와 동일할 때\n",
    "            words.append(list(word_centroid_map.keys())[i])   # 해당 위치의 단어를 1~10 클러스터 순으로 붙임\n",
    "    print (words)                                             # 결국 각 단어들을 1~10번대의 클러스터 순으로 재정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    \n",
    "    # 클러스터 개수는 사전에서 가장 높은 클러스터의 인덱스와 같다.\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1  # 0부터 시작하니 1을 더해줌\n",
    "    \n",
    "    # 속도를 위해 bag of centroids 벡터에 0을 사전할당 \n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    \n",
    "    # 리뷰에서 단어들을 루프 돌린다. 단어가 vocab에 있다면, 이 단어가 속한 클러스터를 찾아 빈도 1을 더함.\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    \n",
    "    # 반환 값은 클러스터 빈도벡터\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    \n",
    "    # document를 list화\n",
    "    # 불용어를 없애는 것은 선택, 보통 높은 질의 워드벡터를 얻기 위해 불용어를 제거하지는 않음.\n",
    "    \n",
    "    # HTML 함수 지우기\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "      \n",
    "    # 글자가 아닌것들 없애기 []안의 ^는 not과 같음.\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    \n",
    "    # 소문자화 후 띄어쓰기를 기준으로 모두 나눔.\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    # 불용어 제거\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # 단어 리스트 반환\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 170 of the file D:\\python\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cleaned reviews\n"
     ]
    }
   ],
   "source": [
    "# 평균을 얻는데 있어서 불용어는 단지 노이즈로 인식되기 때문에 불용어를 삭제한다.\n",
    "clean_train_reviews = []\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist(review, remove_stopwords=True))\n",
    "\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist(review, remove_stopwords=True))\n",
    "\n",
    "print (\"Creating cleaned reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 속도를 위해 bag of centroids에 0을 미리 할당.\n",
    "# train셋에 있는 리뷰들을 bag of centroid로 변환\n",
    "\n",
    "train_centroids = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1\n",
    "\n",
    "# test셋에 대해서도 반복\n",
    "test_centroids = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# 랜덤포레스트 훈련 및 예측\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "print (\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "# 테스트셋 결과 csv 파일로 저장\n",
    "\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment\n",
       "0  \"12311_10\"          1\n",
       "1    \"8348_2\"          0\n",
       "2    \"5828_4\"          1\n",
       "3    \"7186_2\"          1\n",
       "4   \"12128_7\"          1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
